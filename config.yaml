
# e.g., "bert-base-uncased", "bert-large-uncased", "bert-base-cased"
bert_model: "bert-base-cased"

# "train" "validation", "dev"
train_split: "train"
validation_split: "validation"

# "ner_tags", "pos_tags", "chunk_tags"
tag_type: "ner_tags"

# AutoTokenizer's argument
nosplits:
  - "[UNK]"
  - "[SEP]"
  - "[PAD]"
  - "[CLS]"
  - "[MASK]"
  - "-LPR-"
  - "-RPR-"

lowercase: false

# pretrained weights for training/evaluation
trained_weights: null

# for searching for nearest neighbor sentence
preprocess_batch_size: 128

# for actual training
train_batch_size: 16

topk: 500

seed: 3435
cuda: -1

# training, AdamW's conf
lr: 5e-5
dropout: 0.1
grad_norm: 1.0
warmup_prop: 0.1
epochs: 10
log_interval: 100
save: null
no_grad_through_neighbors: true

train_num_neighbor_sentences: 50
eval_num_neighbor_sentences: 100
random_neighbors_in_train: true

eval_accuracy: false

shard_batch_size: 64
cosine: false

# "sum", "first", "last"
wordpiece_word_mapping: "first"

max_num_neighbor_tokens: 2500

# perform evaluation only (with pretrain or finetuned BERT params)
eval_only: false
